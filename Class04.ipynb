{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGkYtRL9TuXzPhFz+3N3p0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/ApplicationsOfDataScienceInDisruptiveTechnologies/blob/main/Class04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MIL (Multiple Instance Learning)**\n",
        "Multiple instance learning (MIL) is a flexible and robust approach to data labeling in machine learning, particularly for dealing with noisy, incomplete, or ambiguous real-world data. MIL operates on the concept of \"bags,\" where each bag contains multiple instances. A bag is labeled positive if at least one instance within it is positive, and negative if all instances are negative. This makes MIL suitable for scenarios where instance-level labeling is difficult, expensive, or even impossible. MIL has proven effective in various applications, such as object detection in images, medical diagnosis, document analysis, and spam filtering. It focuses on global patterns within a bag, rather than being overly influenced by potentially noisy or mislabeled individual instances. MIL implementation typically involves using traditional machine learning algorithms, but the goal is to minimize classification error in bags. Platforms like Google Colab provide an accessible environment for experimenting with MIL, allowing users to build and train models using real-world datasets and share code and results with other researchers and professionals. As the amount of data continues to grow exponentially, MIL will likely play an increasingly important role in extracting meaningful insights from complex and challenging data."
      ],
      "metadata": {
        "id": "mcgHiQL1BWtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "Y1NMHqiNCRtU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MIL on Google Colab**"
      ],
      "metadata": {
        "id": "9ule16QhFll5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4DRzKsFBUTJ",
        "outputId": "60a15dbb-8b5c-467b-d21e-920058d4090e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the optimized Random Forest model: 0.60\n",
            "ROC AUC of the optimized Random Forest model: 0.65\n",
            "Accuracy of Gradient Boosting model: 0.60\n",
            "ROC AUC of Gradient Boosting model: 0.63\n",
            "Accuracy of modelo XGBoost model: 0.63\n",
            "ROC AUC of XGBoost model: 0.66\n"
          ]
        }
      ],
      "source": [
        "# Function to create bags with more informative features and instances:\n",
        "def create_bags(num_bags=100, num_instances_per_bag=20):\n",
        "    X, y = [], []\n",
        "    num_positive_bags = num_bags // 2\n",
        "    num_negative_bags = num_bags - num_positive_bags\n",
        "\n",
        "    for _ in range(num_positive_bags):\n",
        "        # Increasing the number of features and make them more informative:\n",
        "        bag = make_classification(n_samples=num_instances_per_bag, n_features=20, n_informative=15, n_redundant=5)\n",
        "        X.append(bag[0])\n",
        "        y.append(1)\n",
        "\n",
        "    for _ in range(num_negative_bags):\n",
        "        bag = make_classification(n_samples=num_instances_per_bag, n_features=20, n_informative=15, n_redundant=5)\n",
        "        X.append(bag[0])\n",
        "        y.append(0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Generaring the bags:\n",
        "X, y = create_bags()\n",
        "\n",
        "# Flatten of the data from the bags to train the model:\n",
        "X_flat = np.array([instance for bag in X for instance in bag])\n",
        "y_flat = np.repeat(y, len(X[0]))\n",
        "\n",
        "# Splitting the data into training and test:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_flat, y_flat, test_size=0.3, stratify=y_flat, random_state=42)\n",
        "\n",
        "# Defining the parameters for optimization with more regulation:\n",
        "param_grid = {\n",
        "    'n_estimators': [100],  # Keeping the estimators\n",
        "    'max_depth': [10],      # Restricting the depth\n",
        "    'min_samples_split': [5, 10],  # Increasing the criteria for division\n",
        "    'min_samples_leaf': [4, 6]     # Additional regularization\n",
        "}\n",
        "\n",
        "# Configuring the GridSearchCV:\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, scoring='roc_auc')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model found by GridSearchCV:\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Making predictions with the best model:\n",
        "y_pred_best_rf = best_model.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the optimized Random Forest model:\n",
        "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
        "print(f'Accuracy of the optimized Random Forest model: {accuracy_best_rf:.2f}')\n",
        "\n",
        "# Evaluating the optimized model with ROC AUC:\n",
        "roc_score_best_rf = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
        "print(f'ROC AUC of the optimized Random Forest model: {roc_score_best_rf:.2f}')\n",
        "\n",
        "# Training the Gradient Boosting model:\n",
        "model_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=10, random_state=42)\n",
        "model_gb.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions with Gradient Boosting:\n",
        "y_pred_gb = model_gb.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of Gradient Boosting ROC AUC:\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "roc_score_gb = roc_auc_score(y_test, model_gb.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(f'Accuracy of Gradient Boosting model: {accuracy_gb:.2f}')\n",
        "print(f'ROC AUC of Gradient Boosting model: {roc_score_gb:.2f}')\n",
        "\n",
        "# Training the XGBoost model:\n",
        "model_xgb = xgb.XGBClassifier(n_estimators=100, max_depth=10, learning_rate=0.1, random_state=42)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions with XGBoost:\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of ROC AUC and XGBoost\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "roc_score_xgb = roc_auc_score(y_test, model_xgb.predict_proba(X_test)[:, 1])\n",
        "\n",
        "print(f'Accuracy of modelo XGBoost model: {accuracy_xgb:.2f}')\n",
        "print(f'ROC AUC of XGBoost model: {roc_score_xgb:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MIL Performance**"
      ],
      "metadata": {
        "id": "sgdeCBibGQwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create bags with multiple instances:\n",
        "def create_bags(num_bags=200, num_instances_per_bag=10):\n",
        "    X, y = [], []\n",
        "    # Defining half of the bags as positive and the other half as negative:\n",
        "    num_positive_bags = num_bags // 2\n",
        "    num_negative_bags = num_bags - num_positive_bags\n",
        "\n",
        "    for _ in range(num_positive_bags):\n",
        "        # Generating data for positive bags:\n",
        "        bag = make_classification(n_samples=num_instances_per_bag, n_features=10, n_informative=5, n_redundant=3)\n",
        "        X.append(bag[0])\n",
        "        y.append(1)\n",
        "\n",
        "    for _ in range(num_negative_bags):\n",
        "        # Generating data for negative bags:\n",
        "        bag = make_classification(n_samples=num_instances_per_bag, n_features=10, n_informative=5, n_redundant=3)\n",
        "        X.append(bag[0])\n",
        "        y.append(0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Creating the bags and instances:\n",
        "X, y = create_bags()\n",
        "\n",
        "# Transforming the data from the bags into a format suitable for the model:\n",
        "X_flat = np.array([instance for bag in X for instance in bag])\n",
        "y_flat = np.repeat(y, len(X[0]))\n",
        "\n",
        "# Splitting the data into training and test:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_flat, y_flat, test_size=0.3, stratify=y_flat, random_state=42)\n",
        "\n",
        "# Training the Random Forest model:\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set:\n",
        "y_pred = model_rf.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the model:\n",
        "accuracy_rf = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of the Random Forest model: {accuracy_rf:.2f}')\n",
        "\n",
        "# Avaliando o modelo com ROC AUC\n",
        "roc_score_rf = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
        "print(f'ROC AUC of the Random Forest model: {roc_score_rf:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q58bmDTuFxCS",
        "outputId": "9c5c830f-79b4-4566-e7cc-e2e62db156fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Random Forest model: 0.55\n",
            "ROC AUC of the Random Forest model: 0.58\n"
          ]
        }
      ]
    }
  ]
}